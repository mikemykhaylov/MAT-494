
@misc{warwick_nash_abalone_1994,
	title = {Abalone},
	url = {https://archive.ics.uci.edu/dataset/1},
	doi = {10.24432/C55C7W},
	publisher = {{UCI} Machine Learning Repository},
	author = {Warwick Nash, Tracy Sellers},
	urldate = {2023-10-22},
	date = {1994},
}

@article{freund_decision-theoretic_1997,
	title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
	volume = {55},
	issn = {00220000},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	pages = {119--139},
	number = {1},
	journaltitle = {Journal of Computer and System Sciences},
	shortjournal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	urldate = {2023-10-22},
	date = {1997-08},
	langid = {english},
}

@incollection{goos_comparison_2003,
	location = {Berlin, Heidelberg},
	title = {A Comparison of Model Aggregation Methods for Regression},
	volume = {2714},
	isbn = {978-3-540-40408-8 978-3-540-44989-8},
	url = {http://link.springer.com/10.1007/3-540-44989-2_10},
	pages = {76--83},
	booktitle = {Artificial Neural Networks and Neural Information Processing — {ICANN}/{ICONIP} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Barutçuoğlu, Zafer and Alpaydın, Ethem},
	editor = {Kaynak, Okyay and Alpaydin, Ethem and Oja, Erkki and Xu, Lei},
	editorb = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2023-10-23},
	date = {2003},
	doi = {10.1007/3-540-44989-2_10},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Submitted Version:/Users/mikemykhaylov/Zotero/storage/PQGCDQUM/Barutçuoğlu and Alpaydın - 2003 - A Comparison of Model Aggregation Methods for Regr.pdf:application/pdf},
}

@article{lin_growth_2005,
	title = {Growth and structure in abalone shell},
	volume = {390},
	issn = {09215093},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921509304008809},
	doi = {10.1016/j.msea.2004.06.072},
	pages = {27--41},
	number = {1},
	journaltitle = {Materials Science and Engineering: A},
	shortjournal = {Materials Science and Engineering: A},
	author = {Lin, Albert and Meyers, Marc André},
	urldate = {2023-10-23},
	date = {2005-01},
	langid = {english},
}

@incollection{djeddi_abalone_2022,
	location = {Cham},
	title = {Abalone Age Prediction Using Machine Learning},
	volume = {1543},
	isbn = {978-3-031-04111-2 978-3-031-04112-9},
	url = {https://link.springer.com/10.1007/978-3-031-04112-9_25},
	pages = {329--338},
	booktitle = {Pattern Recognition and Artificial Intelligence},
	publisher = {Springer International Publishing},
	author = {Guney, Seda and Kilinc, Irem and Hameed, Alaa Ali and Jamil, Akhtar},
	editor = {Djeddi, Chawki and Siddiqi, Imran and Jamil, Akhtar and Ali Hameed, Alaa and Kucuk, İsmail},
	urldate = {2023-10-23},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-04112-9_25},
	note = {Series Title: Communications in Computer and Information Science},
	file = {Guney et al. - 2022 - Abalone Age Prediction Using Machine Learning.pdf:/Users/mikemykhaylov/Zotero/storage/BMWKIZXN/Guney et al. - 2022 - Abalone Age Prediction Using Machine Learning.pdf:application/pdf},
}

@inproceedings{misman_prediction_2019,
	title = {Prediction of Abalone Age Using Regression-Based Neural Network},
	url = {https://ieeexplore.ieee.org/abstract/document/8970983},
	doi = {10.1109/AiDAS47888.2019.8970983},
	abstract = {Artificial neural networks ({ANN}) has been widely used to speed up data prediction operations with over thousands of features available. In this paper, we propose a regression-based {ANN} model with three hidden layers to predict the age of abalones. It is salient to predict abalone age as it helps farmers and sellers to determine the market price of abalones. The economic value of abalone is positively correlated with their respective ages. The age of the abalone can be estimated by measuring the number of layers of shell rings The model was built based on a dataset obtained from the {UCI} Machine Learning Repository. Before developing and training the model, a pre-processing methodology was applied to the dataset. Parameters tuning, which involves modifications in the number of hidden layers as well as the number of epochs, were done to obtain the best result. The finalised results were analysed and the results show that physical measurements of abalone can predict its respective age with less time consumption. This study has shown a result of low root mean-squared error, obtained from the proposed model in comparison with other methods stated in this study. Finally, the proposed model was validated using test dataset, and the results reveal a lower root-mean-squared error value in contrast to the value obtained during model training.},
	eventtitle = {2019 1st International Conference on Artificial Intelligence and Data Sciences ({AiDAS})},
	pages = {23--28},
	booktitle = {2019 1st International Conference on Artificial Intelligence and Data Sciences ({AiDAS})},
	author = {Misman, Muhammad Faiz and Samah, Azurah A and Aziz, Nur Azni Ab and Majid, Hairudin Abdul and Shah, Zuraini Ali and Hashim, Haslina and Harun, Muhamad Farhin},
	urldate = {2023-11-25},
	date = {2019-09},
	file = {IEEE Xplore Abstract Record:/Users/mikemykhaylov/Zotero/storage/9BLYYJBG/8970983.html:text/html;IEEE Xplore Full Text PDF:/Users/mikemykhaylov/Zotero/storage/XG5J3IMW/Misman et al. - 2019 - Prediction of Abalone Age Using Regression-Based N.pdf:application/pdf},
}

@article{wang_abalone_nodate,
	title = {Abalone Age Prediction Employing A Cascade Network Algorithm and Conditional Generative Adversarial Networks},
	abstract = {Many machine learning methods are applied to solve classiﬁcation tasks, such as {SVM}, Decision Tree, K-Nearest Neighbor. Unfortunately, most of these methods rely heavily on handicraft features and perform poorly on the complex dataset. Artiﬁcial Neural Network({ANN}) has been proved to be a powerful methodology to build a model. Cascade Correlation (Cascor), a promising dynamic network algorithm, works eﬃciently and eﬀectively on both regression and classiﬁcation tasks. We ﬁrst build a simple feedforward network a biological classiﬁcation task, abalone age prediction. Then apply Cascor algorithm, and a variant of Cascor, Cascor employing Progressive {RPROP}(Casper), to improve its performance. The experimental results show that all these neural network solutions perform better than Decision Tree on this task. In the end, we also apply Conditional Generative Adversarial Nets to generate training data, which aﬀects Casper’s performance.},
	author = {Wang, Zhengjie},
	langid = {english},
	file = {Wang - Abalone Age Prediction Employing A Cascade Network.pdf:/Users/mikemykhaylov/Zotero/storage/4LFJ3BDA/Wang - Abalone Age Prediction Employing A Cascade Network.pdf:application/pdf},
}

@article{rifkin_notes_nodate,
	title = {Notes on Regularized Least-Squares},
	abstract = {This is a collection of information about regularized least squares ({RLS}). The facts here are not “new results”, but we have not seen them usefully collected together before. A key goal of this work is to demonstrate that with {RLS}, we get certain things “for free”: if we can solve a single supervised {RLS} problem, we can search for a good regularization parameter λ at essentially no additional cost.},
	author = {Rifkin, Ryan M and Lippert, Ross A},
	langid = {english},
	file = {Rifkin and Lippert - Notes on Regularized Least-Squares.pdf:/Users/mikemykhaylov/Zotero/storage/AW3AM4U7/Rifkin and Lippert - Notes on Regularized Least-Squares.pdf:application/pdf},
}

@article{kim_interior-point_2007,
	title = {An Interior-Point Method for Large-Scale -Regularized Least Squares},
	volume = {1},
	issn = {1932-4553, 1941-0484},
	url = {http://ieeexplore.ieee.org/document/4407767/},
	doi = {10.1109/JSTSP.2007.910971},
	abstract = {Recently, a lot of attention has been paid to 1 regularization based methods for sparse signal reconstruction (e.g., basis pursuit denoising and compressed sensing) and feature selection (e.g., the Lasso algorithm) in signal processing, statistics, and related ﬁelds. These problems can be cast as 1-regularized least-squares programs ({LSPs}), which can be reformulated as convex quadratic programs, and then solved by several standard methods such as interior-point methods, at least for small and medium size problems. In this paper, we describe a specialized interior-point method for solving large-scale 1-regularized {LSPs} that uses the preconditioned conjugate gradients algorithm to compute the search direction. The interior-point method can solve large sparse problems, with a million variables and observations, in a few tens of minutes on a {PC}. It can efﬁciently solve large dense problems, that arise in sparse signal recovery with orthogonal transforms, by exploiting fast algorithms for these transforms. The method is illustrated on a magnetic resonance imaging data set.},
	pages = {606--617},
	number = {4},
	journaltitle = {{IEEE} Journal of Selected Topics in Signal Processing},
	shortjournal = {{IEEE} J. Sel. Top. Signal Process.},
	author = {Kim, Seung-Jean and Koh, K. and Lustig, M. and Boyd, Stephen and Gorinevsky, Dimitry},
	urldate = {2023-11-26},
	date = {2007-12},
	langid = {english},
	file = {Kim et al. - 2007 - An Interior-Point Method for Large-Scale -Regulari.pdf:/Users/mikemykhaylov/Zotero/storage/LTZ7C3H7/Kim et al. - 2007 - An Interior-Point Method for Large-Scale -Regulari.pdf:application/pdf},
}

@article{zou_degrees_2007,
	title = {On the "degrees of freedom" of the lasso},
	volume = {35},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/0712.0881},
	doi = {10.1214/009053607000000127},
	abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein's unbiased risk estimation ({SURE}). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso--a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria--\$C\_p\$, {AIC} and {BIC}--are available, which, along with the {LARS} algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
	number = {5},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	urldate = {2023-11-26},
	date = {2007-10-01},
	eprinttype = {arxiv},
	eprint = {0712.0881 [math, stat]},
	keywords = {62J05, 62J07, 90C46 (Primary), Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/mikemykhaylov/Zotero/storage/MWIYX2PS/Zou et al. - 2007 - On the degrees of freedom of the lasso.pdf:application/pdf;arXiv.org Snapshot:/Users/mikemykhaylov/Zotero/storage/4TVC5UNM/0712.html:text/html},
}

@article{drucker_improving_nodate,
	title = {Improving Regressors using Boosting Techniques},
	abstract = {In the regression context, boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor. We use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines. Performance is analyzed on three non-linear functions and the Boston housing database. In all cases, boosting is at least equivalent, and in most cases better than bagging in terms of prediction error.},
	author = {Drucker, Harris},
	langid = {english},
	file = {Drucker - Improving Regressors using Boosting Techniques.pdf:/Users/mikemykhaylov/Zotero/storage/I35JK6QV/Drucker - Improving Regressors using Boosting Techniques.pdf:application/pdf},
}

@inproceedings{mckinney_data_2010,
	location = {Austin, Texas},
	title = {Data Structures for Statistical Computing in Python},
	url = {https://conference.scipy.org/proceedings/scipy2010/mckinney.html},
	doi = {10.25080/Majora-92bf1922-00a},
	eventtitle = {Python in Science Conference},
	pages = {56--61},
	author = {{McKinney}, Wes},
	urldate = {2023-11-26},
	date = {2010},
	file = {Full Text:/Users/mikemykhaylov/Zotero/storage/QF9USPK9/McKinney - 2010 - Data Structures for Statistical Computing in Pytho.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Abstract
            
              Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. {NumPy} is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, {NumPy} was an important part of the software stack used in the discovery of gravitational waves
              1
              and in the first imaging of a black hole
              2
              . Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. {NumPy} is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own {NumPy}-like interfaces and array objects. Owing to its central position in the ecosystem, {NumPy} increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface ({API}), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	pages = {357--362},
	number = {7825},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and Van Der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	urldate = {2023-11-26},
	date = {2020-09-17},
	langid = {english},
	file = {Full Text:/Users/mikemykhaylov/Zotero/storage/NCR4BUJD/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	shorttitle = {Scikit-learn},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and {API} consistency. It has minimal dependencies and is distributed under the simplified {BSD} license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	pages = {2825--2830},
	number = {85},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	urldate = {2023-11-26},
	date = {2011},
	file = {Full Text PDF:/Users/mikemykhaylov/Zotero/storage/SAYEWNYM/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: A 2D Graphics Environment},
	volume = {9},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/4160265/},
	doi = {10.1109/MCSE.2007.55},
	shorttitle = {Matplotlib},
	pages = {90--95},
	number = {3},
	journaltitle = {Computing in Science \& Engineering},
	shortjournal = {Comput. Sci. Eng.},
	author = {Hunter, John D.},
	urldate = {2023-11-26},
	date = {2007},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	shorttitle = {{PyTorch}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. {PyTorch} is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as {GPUs}. In this paper, we detail the principles that drove the implementation of {PyTorch} and how they are reflected in its architecture. We emphasize that every aspect of {PyTorch} is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of {PyTorch} on several common benchmarks.},
	number = {{arXiv}:1912.01703},
	publisher = {{arXiv}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and {DeVito}, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	urldate = {2023-11-26},
	date = {2019-12-03},
	eprinttype = {arxiv},
	eprint = {1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mikemykhaylov/Zotero/storage/T9SW8KB4/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/mikemykhaylov/Zotero/storage/5GQXX8B5/1912.html:text/html},
}

@online{mykhaylov_general_nodate,
	title = {General · mikemykhaylov/mat422Coursework},
	url = {https://github.com/mikemykhaylov/mat422Coursework},
	abstract = {Mathematical Methods in Data Science Coursework 📖. Contribute to mikemykhaylov/mat422Coursework development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	author = {Mykhaylov, Michael},
	urldate = {2023-11-27},
	langid = {english},
	file = {Snapshot:/Users/mikemykhaylov/Zotero/storage/NM4FW8LN/mat422Coursework.html:text/html},
}
